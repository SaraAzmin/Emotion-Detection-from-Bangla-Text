from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import MultinomialNB
#from ruleparser import RuleParser
from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support, ConfusionMatrixDisplay
import re
from sklearn.svm import SVC
from sklearn.metrics import classification_report
from sklearn.model_selection import cross_val_score, GridSearchCV
import matplotlib.pyplot as plt

# Importing Files for POS Tagged Data
import Viterbi_POS_Tagger.HMM as HMM
import Viterbi_POS_Tagger.Apply_POS as Apply_POS

'''
Reads the text corpus (dataset).
use_sentiment (true) for binary sentiment class (pos/neg)
'''
roots=[];
dicbased=[];
rulebased=[];

labelDict={"angry":1, "disgust":2, "fear":3, "happy":4, "sad":5, "surprise":6 }

uposorgoStrings=["ржЕ","ржЕржШрж╛","ржЕржЬ","ржЕржирж╛","ржЖ","ржЖрзЬ","ржЖржи","ржЖржм","ржЗрждрж┐","ржКржи","ржКржирж╛","ржХржж","ржХрзБ","ржирж┐","ржкрж╛рждрж┐","ржмрж┐","ржнрж░","рж░рж╛ржо","рж╕",
                 "рж╕рж╛","рж╕рзБ","рж╣рж╛ ","ржкрзНрж░","ржкрж░рж╛","ржЕржк","рж╕ржо","ржирж┐","ржЕржирзБ","ржЕржм","ржирж┐рж░","ржжрзБрж░","ржмрж┐","ржЕржзрж┐","рж╕рзБ","ржЙрзО","ржкрж░рж┐","ржкрзНрж░рждрж┐","ржЕрждрж┐","ржЕржкрж┐",
                 "ржЕржнрж┐","ржЙржк","ржЖ","ржХрж╛рж░","ржжрж░","ржирж┐ржо","ржлрж┐","ржмржж","ржмрзЗ","ржмрж░","ржм","ржХржо","ржЖржо","ржЦрж╛рж╕","рж▓рж╛","ржЧрж░","ржлрзБрж▓","рж╣рж╛ржл","рж╣рзЗржб","рж╕рж╛ржм","рж╣рж░"];

prottoi=["ржорзБржХрзНржд","ржЧрж╛ржорзА","ржЧрж╛ржорзАрж░","ржЪрж╛рж░рзА","ржЪрж╛рж░рзАрж░","ржмрж╛рж╣рзА","ржмрж╛рж╣рзАрж░","рж╕ржорзЗржд","ржоржгрзНржбрж▓рзА","ржоржгрзНржбрж▓рзАрж░",
	"ржХрж░рзНржорж╛","ржХрж░рзНржорж╛рж░","ржХрж╛рж░рзНржп","ржкрзВрж░рзНржм","ржХрж░","ржХрж░рж╛","ржХрж░рж╛рж░","ржорж╛рж▓рж╛","ржорж╛рж▓рж╛рж░","ржХрзАрж░рзНрждрж┐","ржХрзАрж░рзНрждрж┐рж░","ржХрж░ржг","рж╛ржВрж╢","ржЧржд","ржЬржиржХ","ржХржгрж╛","ржХржгрж╛рж░","рж╕рж╣",
	"ржорзВрж▓ржХ","ржнрж╛ржмрзЗ","ржпрзЛржЧрзНржп","ржнрж┐рждрзНрждрж┐ржХ","рж╕рзВржЪржХ","ржХрж╛рж░рзА","ржХрж╛рж░рзАрж░","ржкрзНрж░ржмржг","ржкрзНрж░ржмржгрждрж╛","ржкрзНрж░ржмржгрждрж╛рж░","рж╕рзБрж▓ржн","ржкрзВрж░рзНржг","рждрзЗ","рждрж╛","ржХрж╛рж▓рзАржи","ржоржд","рждрзНржм","ржкрждрзНрж░",
	"ржорзБржЦрзА","ржорзБржЦрзАрж░","рж╕ржорзНржкржирзНржи","ржорзЯ","рж╕ржВржХрзНрж░рж╛ржирзНржд","рж╕ржорзГржжрзНржз","ржжрж╛рж░","ржнрж░рж╛","ржнрж░рж╛рж░","ржЧрзНрж░рж╕рзНржд","ржЬржирж┐ржд","ржЬрзАржмрзА","ржЬрзАржмрзАрж░","ржирзАрждрж┐","ржирзАрждрж┐рж░","ржмрж┐ржзрж┐","ржмрж┐ржзрж┐рж░","ржмрж╛ржЬ","ржмрж┐ржорзБржЦ",
	"рж╢рзАрж▓","ржмрж╛ржжрзА","ржмрж╛ржжрзАрж░","ржмрж╛ржи","ржЧрж╛ржЫрж┐","ржЧрж╛ржЫрж┐рж░","ржирж╛ржорж╛","ржирж╛ржорж╛рж░","ржЬржи","ржмрж╣","ржмрж╛рж░","ржзрж╛рж░рзА","ржзрж╛рж░рзАрж░","ржжрж╛рзЯржХ","ржжрж╛рждрж╛","ржжрж╛рждрж╛рж░","ржХрж╛рж▓","рж╕рзВржЪрж┐","рж╕рзВржЪрж┐рж░","ржЧрж╛рж░","ржнрзЛржЧрзА","ржнрзЛржЧрзАрж░", "ржнрж╛ржЧрзА","ржнрж╛ржЧрзАрж░", 
	"ржорж╛рждрзНрж░","ржмрж┐ржж","ржмрж╛ржж","ржХржгрж╛","ржнржХрзНржд","ржнрзБржХрзНржд","ржнрзБржХрзНрждрж┐","ржнрзБржХрзНрждрж┐рж░","ржнрзБржХрзНрждрж┐рждрзЗ","ржкржирзНржерж╛","ржкржирзНржерзА","ржкржирзНржерзАрж░","ржХрж╛ржорзА","ржХрж╛ржорзАрж░","ржпрзЛржЧ","ржпрзЛржЧрзЗ","ржмрзЛржз","ржкрзНрж░рждрж┐",
	"ржХрж╛ржЬ","рждржо","ржмрж┐рж╖рзЯржХ","рж╕рж╛ржоржЧрзНрж░рзА","рж╕рж╛ржоржЧрзНрж░рзАрж░","ржЬрж╛ржд","ржмрж╛ржЬрж┐","ржмрж╛ржЬрж┐рж░","ржХржмрж▓рж┐ржд","рж╕рзВрждрзНрж░рзЗ","ржЬрж╛ржд","ржмржжрзНржз",
	"ржЯрж╛","ржЯрж┐","ржЯрзБ","ржЯрж┐рж░","ржЯрж╛рж░","ржЯрзБржХрзБ","ржЯрзБржХрзБрж░","ржзрж░рзНржорзА","ржзрж░рзНржорзАрж░","ржпрзБржХрзНржд","ржХрзГржд","ржмрзНржпрж╛ржкрзА","ржмрзНржпрж╛ржкрзАрж░","ржХрзЗржирзНржжрзНрж░рж┐ржХ","ржмрж┐рж░рзЛржзрзА","ржмрж┐рж░рзЛржзрзАрж░","ржорзБржХрзНрждрж┐","ржорзБржХрзНрждрж┐рж░",
	"рж░рзВржкрзА","рж░рзВржкрзАрж░","рж░рзВржкрзЗ","рж░рзВржк","рж╢рж╛рж▓рж┐","рж╢рж╛рж▓рж┐рж░","рж╢рж╛рж▓рзА","рж╢рж╛рж▓рзАрж░","ржкрзНрж░рж╕рзВржд","ржмрж╛рж╕рзА","ржмрж╛рж╕рзАрж░","ржкрзНрж░ржмрж╛рж╕рзА","ржкрзНрж░ржмрж╛рж╕рзАрж░","ржкрзНрж░рж╛ржкрзНржд","ржЧрзЛрж╖рзНржарзА","ржЧрзЛрж╖рзНржарзАрж░","ржмрж╛ржЪржХ","ржирж┐рж░рзНржнрж░","рж╕рзНржмрж░",
	"рж╕ржВрж▓ржЧрзНржи","рж╢рж╛рж╕рзНрждрзНрж░","ржорж╛ржлрж┐ржХ","рж╕рзНржмрж░рзВржк","ржХрзВрж▓","рж╕ржорзНржоржд","рж╕рж┐ржжрзНржз","ржмрзГржирзНржж","ржжрж╛ржи","рж╕рж╣","ржмрж╢ржд","ржнрж░", 
	#i
	"ржЗ","ржУ","ржПрж░","ржП",
	#prottoi
	"рзЯрзЗ","ржирж╛","рждрж┐","рждрж╛","ржУрзЯрж╛","ржЙрзЯрж╛","ржЙржирж┐","ржЙржХрж╛","ржИ","ржЖржУ","ржЕ"," рж░рж┐"," рждрж┐"," рждрж╛","рждрзЗ", " ржП"," ржК",
	" ржЙрж░рж┐"," ржЙржирзНрждрж┐"," ржЙржХ"," ржЗрзЯрзЗ"," ржЗрзЯрж╛"," ржЗрж▓рзЗ"," ржЖрж▓",
	" ржЖрж░рзА"," ржЖрж░рж┐"," ржЖржирзЛ"," ржЖржирж┐"," ржЖржи"," ржЖржд"," ржЖржХрзБ"," ржЖржЗржд"," ржЖржЗ"," ржЖ"," ржЕржирзНрждрж┐"," ржЕржирзНржд"," ржЕржирж┐",
	" ржЕржирж╛"," ржЕржи"," ржЕрждрж┐"," ржЕрждрж╛"," ржЕржд"," ржЕржХ",
	" рзЛрзЯрж╛","рзБрзЯрж╛","рзБржирзНрждрж┐","рзБржирж┐","рзБржХрж╛","рзБржХрж╛","рзБ","рзЛ","рзЗ","рзА","рзАрзЯ",
         #/*"рж┐",*/
         "рзЯ",
	# prottoi-bangla(toddit)
	"рзЬрзЗ","рзЬрж┐","рзЬрж╛","рж╕рзЗ","рж╕рж┐рзЯ","рж╕рж╛","рж▓рж╛","ржорж┐","ржоржирзНржд","ржнрж░рж╛","ржнрж░","ржкрж╛рж░рж╛","ржкрж╛ржирзЛ","ржкржирж╛","ржирж┐","рждрзБржд","рждрж┐","рждрж╛",
	"ржЯрзЗ","ржЯрж┐рзЯрж╛","ржЯрж╛","ржЪрзЗ","ржХрзЗ","ржХрж┐рзЯрж╛","ржХрж╛","ржХ","ржУрзЯрж╛рж▓рж╛","ржПрж▓","ржПрж▓","ржК","ржЙрзЯрж╛","ржЙрзЬрж┐рзЯрж╛","ржЙрзЬ","ржЙрж▓рж┐","ржЙрж░рзЗ",
	"ржЙрж░рж┐рзЯрж╛","ржЙржХ","ржЗрзЯрж╛рж▓","ржЗрзЯрж╛","ржЗ","ржЖрж▓рзЛ","ржЖрж▓рж┐","ржЖрж▓рж╛","ржЖрж▓","ржЖрж░рзБ","ржЖрж░рж┐","ржЖрж░","ржЖржорзЛ","ржЖржорж┐","ржЖржо",
	"ржЖржирзЛ","ржЖржирж┐","ржЖржд","ржЖржЪрж┐","ржЖржЪ","ржЖржЗржд","ржЖржЗ","ржЖ","ржЕрзЬ","ржЕрж▓",
	#case
	"рзЗрж░","рзЯрзЗржжрзЗрж░ржХрзЗ","рзЗржжрзЗрж░ржХрзЗ","ржжрзЗрж░ржХрзЗ","рзЯрзЗржжрзЗрж░","рзЗржжрзЗрж░","рзЯрзЗрж░рж╛","ржнрж╛ржмрзЗ","ржжрзЗрж░","рзЗрж░рж╛","рзЛрж░","рж╛рж░","рзЯрзЗрж░",
	#/*"ржХрж╛рзЯ",*/
        "ржХрж╛рж░","рж┐рж▓рж╛","рж┐рж▓рж╛","рж┐ржд","рж╛ржи","ржХрзБ","ржХрж╛","рждрзЗ","рж░рж╛","рзЯ","ржорж┐",
        #/*"рж╕рж┐","рзЬрзЗ",*/
        "ржХрзЗ","рзЗржЗ","рж░","рж╛","рж╛ржирзБ","ржмрзГржирзНржж",
        #"рзЯ","ржм","ржо","рж╕",
	#article 
	"ржЦрж╛ржирж╛","ржЦрж╛ржирж╛рж░","ржЦрж╛ржирж╛рзЯ","ржЦрж╛ржирж┐","ржЦрж╛ржирж┐рж░","ржЧрзБрж▓рзЛ","ржЧрзБрж▓рзЛ","ржЧрзБрж▓рзЛрж░","ржЧрзБрж▓рзЛрзЯ","ржЧрзБрж▓рзЛрждрзЗ","ржЧрзБрж▓рж┐","ржЧрзБрж▓рж┐рж░","ржЧрзБрж▓рж┐рждрзЗ","рзЯрзЛржи","ржЦрж╛ржи","ржЧрзБрж▓","рж╕ржорзВрж╣","ржЧржг",
	"ржЯрж╛","ржЯрж┐","ржЯрзБ","ржЯрж┐рж░","ржЯрж╛рж░","ржЯрзБржХрзБ",
	"ржХрж╛","рждрж╛","рж┐рж▓","рж┐ржХ",	"рзЗржХ","рзЗржд",
        #/*"рж▓рж┐",*/
        "рзЯрж╛","рж╛рзЯ","рзЬрж┐","рж┐рж╕","рж╛ржи",
	"рж╣рзАржирждрж╛","рж╣рзАржи","ржмрж┐рж╣рзАржи",
	#verb
	"ржЫрж┐рж╕", "ржЫрзЗ", "ржЫрзЗржи", "ржЪрзНржЫрж┐", "ржЪрзНржЫ", "ржЪрзНржЫрзЗ","ржЪрзНржЫрзЗржи", 
	"рзЗржЫрж┐","рзЗржЫ","рзЗржЫрзЗ","рзЗржЫрзЗржи",
	"рж╛ржЫрж┐","рж╛ржЫ","рж╛ржЫрзЗ","рж╛ржЫрзЗржи",
	"рж┐рзЯ","рж┐",			
	"рзЯрж╛ржЫ","рзЯрж╛ржЫрж┐","рзЯрж╛ржЫрзЗ","рзЯрж╛ржЫрзЗржи",
	"рж▓рж╛ржо","рж▓рзЗржо","рж▓рзБржо","рж▓рзЗржи", "ржЫрж┐рж▓рзЗ", "ржЫрж┐рж▓", "рждрзЛ", 
	"рждрж╛ржо","рждрзЗржо","рждрзБржо", "рждрзЗржи", "рж▓",
	"ржмрзЗржи", "ржмрзЛ","ржмрзЗ","ржмрж┐", "рзЗржи",
	"рж╛ржо","рзЗржо","рзБржо",
	"рж┐рзЯрзЗ",
	"рж╛ржЗ", 
	#normalization
	"рж┐ржирж┐","ржирж┐","ржирж╛","рждрзЗржЗ","ржЯрж╛ржЗ","рж╕рзНрже","рж╛ржпрж╝ржи","рж╛ржЪрзНржЫржирзНржи",#/*"ржХ",*/
        "рждрзЗ" #//normalize
        ];


must=["ржЗ","ржУ","ржПрж░","ржП","рзЗрж░","рзЯрзЗржжрзЗрж░ржХрзЗ","рзЗржжрзЗрж░ржХрзЗ","ржжрзЗрж░ржХрзЗ","рзЯрзЗржжрзЗрж░","рзЗржжрзЗрж░","рзЯрзЗрж░рж╛","ржнрж╛ржмрзЗ","ржжрзЗрж░","рзЗрж░рж╛","рзЯрзЗрж░","рж░рж╛","ржХрзЗ",
     "ржЦрж╛ржирж╛","ржЦрж╛ржирж╛рж░","ржЦрж╛ржирж╛рзЯ","ржЦрж╛ржирж┐","ржЦрж╛ржирж┐рж░","ржЧрзБрж▓рзЛ","ржЧрзБрж▓рзЛ","ржЧрзБрж▓рзЛрж░","ржЧрзБрж▓рзЛрзЯ","ржЧрзБрж▓рзЛрждрзЗ","ржЧрзБрж▓рж┐","ржЧрзБрж▓рж┐рж░","ржЧрзБрж▓рж┐рждрзЗ","рзЯрзЛржи","ржЦрж╛ржи","ржЧрзБрж▓",
     "рж╕ржорзВрж╣","ржЧржг", 
     "ржорзБржХрзНржд","ржЧрж╛ржорзА","ржЧрж╛ржорзАрж░","ржЪрж╛рж░рзА","ржЪрж╛рж░рзАрж░","ржмрж╛рж╣рзА","ржмрж╛рж╣рзАрж░","рж╕ржорзЗржд","ржоржгрзНржбрж▓рзА","ржоржгрзНржбрж▓рзАрж░",
     "ржХрж░рзНржорж╛","ржХрж░рзНржорж╛рж░","ржХрж╛рж░рзНржп","ржкрзВрж░рзНржм","ржХрж░","ржХрж░рж╛","ржХрж░рж╛рж░","ржорж╛рж▓рж╛","ржорж╛рж▓рж╛рж░","ржХрзАрж░рзНрждрж┐","ржХрзАрж░рзНрждрж┐рж░","ржХрж░ржг","рж╛ржВрж╢","ржЧржд","ржЬржиржХ","ржХржгрж╛","ржХржгрж╛рж░","рж╕рж╣",
     "ржорзВрж▓ржХ","ржнрж╛ржмрзЗ","ржпрзЛржЧрзНржп","ржнрж┐рждрзНрждрж┐ржХ","рж╕рзВржЪржХ","ржХрж╛рж░рзА","ржХрж╛рж░рзАрж░","ржкрзНрж░ржмржг","ржкрзНрж░ржмржгрждрж╛","ржкрзНрж░ржмржгрждрж╛рж░","рж╕рзБрж▓ржн","ржкрзВрж░рзНржг","рждрзЗ","рждрж╛","ржХрж╛рж▓рзАржи","ржоржд","рждрзНржм","ржкрждрзНрж░",
     "ржорзБржЦрзА","ржорзБржЦрзАрж░","рж╕ржорзНржкржирзНржи","ржорзЯ","рж╕ржВржХрзНрж░рж╛ржирзНржд","рж╕ржорзГржжрзНржз","ржжрж╛рж░","ржнрж░рж╛","ржнрж░рж╛рж░","ржЧрзНрж░рж╕рзНржд","ржЬржирж┐ржд","ржЬрзАржмрзА","ржЬрзАржмрзАрж░","ржирзАрждрж┐","ржирзАрждрж┐рж░","ржмрж┐ржзрж┐","ржмрж┐ржзрж┐рж░","ржмрж╛ржЬ","ржмрж┐ржорзБржЦ",
     "рж╢рзАрж▓","ржмрж╛ржжрзА","ржмрж╛ржжрзАрж░","ржмрж╛ржи","ржЧрж╛ржЫрж┐","ржЧрж╛ржЫрж┐рж░","ржирж╛ржорж╛","ржирж╛ржорж╛рж░","ржЬржи","ржмрж╣","ржмрж╛рж░","ржзрж╛рж░рзА","ржзрж╛рж░рзАрж░","ржжрж╛рзЯржХ","ржжрж╛рждрж╛","ржжрж╛рждрж╛рж░","ржХрж╛рж▓","рж╕рзВржЪрж┐","рж╕рзВржЪрж┐рж░","ржЧрж╛рж░","ржнрзЛржЧрзА","ржнрзЛржЧрзАрж░", "ржнрж╛ржЧрзА","ржнрж╛ржЧрзАрж░", 
     "ржорж╛рждрзНрж░","ржмрж┐ржж","ржмрж╛ржж","ржХржгрж╛","ржнржХрзНржд","ржнрзБржХрзНржд","ржнрзБржХрзНрждрж┐","ржнрзБржХрзНрждрж┐рж░","ржнрзБржХрзНрждрж┐рждрзЗ","ржкржирзНржерж╛","ржкржирзНржерзА","ржкржирзНржерзАрж░","ржХрж╛ржорзА","ржХрж╛ржорзАрж░","ржпрзЛржЧ","ржпрзЛржЧрзЗ","ржмрзЛржз","ржкрзНрж░рждрж┐",
     "ржХрж╛ржЬ","рждржо","ржмрж┐рж╖рзЯржХ","рж╕рж╛ржоржЧрзНрж░рзА","рж╕рж╛ржоржЧрзНрж░рзАрж░","ржЬрж╛ржд","ржмрж╛ржЬрж┐","ржмрж╛ржЬрж┐рж░","ржХржмрж▓рж┐ржд","рж╕рзВрждрзНрж░рзЗ","ржЬрж╛ржд","ржмржжрзНржз",
     "ржЯрж╛","ржЯрж┐","ржЯрзБ","ржЯрж┐рж░","ржЯрж╛рж░","ржЯрж╛рзЯ","ржЯрзБржХрзБ","ржЯрзБржХрзБрж░","ржзрж░рзНржорзА","ржзрж░рзНржорзАрж░","ржпрзБржХрзНржд","ржХрзГржд","ржмрзНржпрж╛ржкрзА","ржмрзНржпрж╛ржкрзАрж░","ржХрзЗржирзНржжрзНрж░рж┐ржХ","ржмрж┐рж░рзЛржзрзА","ржмрж┐рж░рзЛржзрзАрж░","ржорзБржХрзНрждрж┐","ржорзБржХрзНрждрж┐рж░",
     "рж░рзВржкрзА","рж░рзВржкрзАрж░","рж░рзВржкрзЗ","рж░рзВржк","рж╢рж╛рж▓рж┐","рж╢рж╛рж▓рж┐рж░","рж╢рж╛рж▓рзА","рж╢рж╛рж▓рзАрж░","ржкрзНрж░рж╕рзВржд","ржмрж╛рж╕рзА","ржмрж╛рж╕рзАрж░","ржкрзНрж░ржмрж╛рж╕рзА","ржкрзНрж░ржмрж╛рж╕рзАрж░","ржкрзНрж░рж╛ржкрзНржд","ржЧрзЛрж╖рзНржарзА","ржЧрзЛрж╖рзНржарзАрж░","ржмрж╛ржЪржХ","ржирж┐рж░рзНржнрж░","рж╕рзНржмрж░",
     "рж╕ржВрж▓ржЧрзНржи","рж╢рж╛рж╕рзНрждрзНрж░","ржорж╛ржлрж┐ржХ","рж╕рзНржмрж░рзВржк","ржХрзВрж▓","рж╕ржорзНржоржд","рж╕рж┐ржжрзНржз","ржмрзГржирзНржж","ржжрж╛ржи","ржмрж╢ржд","ржнрж░",
     "рж╕рж╣","рзЗ","рзЗрж░","рзЗрзЯ","рзАрзЯ","рзЗржХ","рж┐ржХ","рж┐рждрж╛","рж┐ржгрзА","рж┐ржирзА","рзА" ];
      
def stemmer( text ):
    WORD_PATTERN=" +|-|'|\b|\t|\n|\f|\r|\"|\'|#|%|&|~|@|<|>|\*|\+|\=|\.|,|\редрз╖|'|\)|\(|\{|\}|\[|\]|;|:|/|\\\\|-|_|тАФ|тАУ|тАЭ|тАЬ";
    st=re.split(WORD_PATTERN,text);
    stemWordList=[];
    roots.clear();
    roots.append(text);
    for str in st :
        if len(str)>0:
            dicbased.clear();
            rulebased.clear();
            stem = findStem(str);
            stemWordList.append(stem);


def findStem( word ):
    rootminstem="";
    dicbased.clear();
    temp="";
    rootminlen = 1000;
    dicbased.append(word);
    rightTrim(word);

    for i in range(0,len(dicbased)):
        temp=dicbased[i];
        length=len(temp);
        if temp not in roots:
            continue;
        if(length>=2 and length<rootminlen):
            rootminstem=temp;
            rootminlen=length;
    if len(rootminstem)>0:
        return rootminstem;
    rulebased.clear();
    rulebased.append(word);
    rightTrimMust(word);

    for i in range(0,len(rulebased)):
        temp=rulebased[i];
        length=len(temp);
        if length>=2 and length<rootminlen:
            rootminstem=temp;
            rootminlen=length;
    return rightTrimRa(rootminstem);
    
    

def rightTrim(word):
    if len(word)<3 :
        return;
    fraction=word;
    for str  in prottoi:
        if(word.endswith(str)):
            fraction = word[0:len(word)-len(str)];
            if(len(fraction)<2):
                continue;
            a="рзН";#0x09CD
            ch=fraction[len(fraction)-1];
            if ch==a:
                continue;
            if (len(word) != len(fraction)):
                dicbased.append(fraction);
                rightTrim(fraction);
    return;

def rightTrimMust( word ):
    if len(word)<3:
        return;
    fraction="";
    if word.endswith("рзЯ") and word.endswith("рж╛рзЯ"):
        word=word[0:len(word)-1];
        rulebased.append(word);

    for i in range(0,len(must)):
        if word.endswith(must[i]):
            fraction=word[0:len(word)-len(must[i])];
            if len(fraction)<2:
                continue;
            a="рзН";#0x09CD
            ch=fraction[len(fraction)-1];
            if ch==a:
                continue;
            if len(word)!=len(fraction):
                rulebased.append(fraction);
                rightTrimMust(fraction);


def rightTrimRa(word):
    if len(word)<3:
        return word;
    fraction=word;
    if word.endswith("рж░"):
        if word.endswith("ржкрзБрж░"):
            return fraction;
        a="рзН"#0x09CD
        b="рж╛"#0x09BE
        c="рзЧ"#0x09D7
        ch=word[len(word)-2];
        if ch>=b and ch<=c and ch!=a:
            fraction=word[0:len(word)-1];
    return fraction;

def read_corpus(corpus_file, use_sentiment):
    documents = []
    labels = []


    waste = '''тАЩ'()[]тЩе{}<>:,тАТтАУтАФтАХтАж.┬л┬╗-тАРтАШтАЩтАЬтАЭ;/тБДтРа┬╖&@*\\тАв^┬д┬в$тВм┬г┬етВйтВктАатАб┬░┬б┬┐┬м#
    тДЦ%тА░тА▒┬╢тА▓┬з~┬и_|┬жтБВтШЮтИ┤тА╜тА╗'"-\ред()/'\"%#/@;:<>{}+=~|тАФтАШтАЩтАЬтАЭ\.!?,`$^&*_+=
    abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789
    рзжрззрзирзйрзкрзлрзмрзнрзорзпЁЯЩВЁЯШАЁЯШДЁЯШЖЁЯШЕЁЯШВЁЯдгЁЯШКтШ║ЁЯШМЁЯШЙЁЯШПЁЯШНЁЯШШЁЯШЧЁЯШЩЁЯШЪЁЯдЧЁЯШ│ЁЯЩГЁЯШЗЁЯШИЁЯШЫЁЯШЭЁЯШЬЁЯШЛЁЯддЁЯдУЁЯШОЁЯдСЁЯШТЁЯЩБ
    тШ╣ЁЯШЮЁЯШФЁЯШЦЁЯШУЁЯШвЁЯШвЁЯШнЁЯШЯЁЯШгЁЯШйЁЯШлЁЯШХЁЯдФЁЯЩДЁЯШдЁЯШаЁЯШбЁЯШ╢ЁЯдРЁЯШРЁЯШСЁЯШпЁЯШ▓ЁЯШзЁЯШиЁЯШ░ЁЯШ▒ЁЯШкЁЯШ┤ЁЯШмЁЯдеЁЯдзЁЯдТЁЯШ╖ЁЯдХЁЯШ╡ЁЯдвЁЯда
    ЁЯдбЁЯС┐ЁЯС╣ЁЯС║ЁЯС╗ЁЯТАЁЯС╜ЁЯС╛ЁЯдЦЁЯТйЁЯОГтЪФЁЯС╣\ЁЯРбЁЯРЯЁЯРаЁЯЩВЁЯШАЁЯШВЁЯСНЁЯШРЁЯШбЁЯП╜тЬКтШЭЁЯЩМЁЯСНЁЯП╗ЁЯСОЁЯП╗тЬМя╕ПЁЯП╗ЁЯдЮЁЯП╗ЁЯСМЁЯП╗ЁЯдЩЁЯП╗ЁЯдШЁЯП╗ЁЯЦХЁЯП╗тШЭ
    ЁЯП╗ЁЯТЕЁЯП╗ЁЯСЙЁЯП╗ЁЯСИЁЯП╗ЁЯСЗЁЯП╗ЁЯСЖЁЯП╗ЁЯТЪЁЯТЩтЭдЁЯТФЁЯТХЁЯТЬЁЯТУЁЯТЮЁЯТЧЁЯТШЁЯТЦЁЯТЭЁЯТЯ'''

    with open(corpus_file, encoding='utf-8-sig') as f:
        for line in f:
            tokens = line.strip().split()
            temp = []
            for char in tokens[1:]:
                no_punct = ""
                for ch in char:
                    if ch not in waste:
                        no_punct = no_punct + ch

                if no_punct:
                    #print(no_punctuation)
                    no_punct = findStem(no_punct)
                    #print(no_punctuation)
                    if no_punct:
                        temp.append(no_punct)
                        
            documents.append(temp)
            if use_sentiment:
                # 2-class problem: positive vs negative
                #for line in data.label:
                #labels.append(labelDict[tokens[0]])
                labels.append(tokens[0])
    #for i in range(11):
     #   print(documents[i])
    return documents, labels



def read_stopwords(stopwords_file):
    stop_words = []
    with open(stopwords_file, encoding='utf-8-sig') as f:
        for line in f:
            stop_words.append(line.strip())

    return stop_words

def remove_stopwords(word_list, stopwords_list):
    processed_word_list = []

    for doc in word_list:
        temp = []
        for word in doc:
            if word not in stopwords_list:
                temp.append(word)

        processed_word_list.append(temp)

    return processed_word_list

# a dummy function that just returns its input
def identity(x):
    return x

def S_V_M_tf(train_data, train_label, test_data, test_label):
    #tfidf = TfidfVectorizer(sublinear_tf=True, min_df=2, norm='l2', encoding='utf-8', ngram_range=(1, 2))

    tfidf = TfidfVectorizer(preprocessor = identity, tokenizer = identity, min_df=2,
                            encoding='utf-8-sig', ngram_range=(1,2))

  #Using Linear SVC
    L_svc=Pipeline([('tfidf',tfidf),('ln_svc', LinearSVC())])
    L_svc.fit(train_data,train_label)
    testGuess=L_svc.predict(test_data)
    #classguess=L_svc.predict(predict)
    L_svc_cm = confusion_matrix(test_label, testGuess, labels=L_svc.classes_)
    prec, rec, f1, tureSum= precision_recall_fscore_support(test_label, testGuess)
    print("SVM with Tf-idf: ")
    #print("predicted class:", classguess)
    accuracy = accuracy_score(test_label, testGuess)
    print("Accuracy = ", accuracy)
    print("Confusion Matrix:")
    print(L_svc.classes_)
    print(L_svc_cm)
    print("Precision: ", prec)
    print("Recall:", rec)
    print("F1 Score: ", f1)
    print("\n") 
    cm_show = ConfusionMatrixDisplay(L_svc_cm, display_labels=['angry', 'happy', 'sad'])
    cm_show.plot()
    cm_show.ax_.set(
                title='Confusion Matrix for SVM', 
                xlabel='Predicted Emotion Class', 
                ylabel='Actual Emotion Class')
    plt.show()

# Fitting SVM to the Training set
    

def Multinomial_Naive_Bayes(trainDoc, trainClass, testDoc, testClass, tfIdf):

    # we use a dummy function as tokenizer and preprocessor,
    # since the texts are already preprocessed and tokenized.
    if tfIdf:
        vec = TfidfVectorizer(preprocessor = identity,
                              tokenizer = identity, ngram_range=(1, 2))
   

    # Pipeline combines the vectorizers with a Naive Bayes classifier
    classifier = Pipeline( [('vec', vec),
                            ('cls', MultinomialNB())] )


    # Train the classifier and build a model using the training documents
    classifier.fit(trainDoc, trainClass)
    
    #saving the model
    from sklearn.externals import joblib
    joblib.dump(classifier, 'NB_model.pkl')
    
    '''
    #Loading the model:
    NB_spam_model = open('NB_model.pkl','rb')
    classifier = joblib.load(NB_spam_model)
    '''
    # Outputs Predicted class for the test set
    testGuess = classifier.predict(testDoc)
    
    try:
        #inputfile = open("input_nb.txt","r", encoding='utf-8')
        valDOC, valLBL = read_corpus('input_nb.txt', use_sentiment=True)
        predz = classifier.predict(valDOC)
        i=0
        for line in valDOC:
            print(line, predz[i])
            i+=1
        #inputfile.close()
    except:
        pass


    prec, rec, f1, tureSum= precision_recall_fscore_support(testClass, testGuess)
    #classguess = classifier.predict(predict)
    #print("predicted class:", classguess)

    # Simply calculates the accuracy score using the Gold Labels and Predicted Labels
    print("Naive Bayes with TF-idf:")
    accuracy = accuracy_score(testClass, testGuess)
    print("Accuracy = "+str(accuracy))
    print("Precision: ", prec)
    print("Recall:", rec)
    print("F1 Score: ", f1)
    #print("\n")

    # Showing the Confusion Matrix
    print("Confusion Matrix:")
    cm = confusion_matrix(testClass, testGuess, labels=classifier.classes_)
    print(classifier.classes_)
    print(cm)


# this is the main function but you can name it anyway you want
def main():
    # reads the whole dataset and separates documents(reviews) and labels(class/categories)
    # value of use sentiment determines whether to use binary(pos/neg) vs six way classification
    #DOC, LBL = read_corpus('rev.txt', use_sentiment=True)
    DOC, LBL = read_corpus('dataset3class.txt', use_sentiment=True)
    
    stop_words = read_stopwords('stopwords-bn.txt')
    DOC = remove_stopwords(DOC, stop_words)

    #print("length of doc =", len(DOC))

    ch = input('Do you want to apply POS-tagger? [Y/N]: ')

    if ch == 'Y' or ch == 'y':
        # This will call the HMM POS tagger. All the paths are specified in Config.py file
        model, vocab = HMM.start()

        # Apply POS Tagging on the above data set
        word_tag = HMM.decoding(model, vocab, DOC)

        # Reduce the features using particular set of POS (all_pos = False) or All pos (all_pos = True)
        # Go and check which POS is used, you can extend/reduce the list of POS to be considered
        DOC = Apply_POS.pos_feature(word_tag, all_pos = True)
    

    print("length of doc =", len(DOC))

    # Splits the dataset into training (90%) and test set(10%)
    split_point = int(0.90*len(DOC))
    trainDoc = DOC[:split_point]
    trainClass = LBL[:split_point]
    testDoc = DOC[split_point:]
    testClass = LBL[split_point:]

    print("Train data: ", len(trainDoc))
    print("Test data: ", len(testDoc))

    #for q in sorted(word2count.keys()):
    #    print(q, end = " ")

    # Calling the classifier (use the tf-idf/count feature/vectorizer)
    S_V_M_tf(trainDoc, trainClass, testDoc, testClass)
    #Multinomial_Naive_Bayes(trainDoc, trainClass, testDoc, testClass, tfIdf=True)
    
    


# program starts from here
if __name__ == '__main__':
    main()

